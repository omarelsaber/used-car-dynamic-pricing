{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd4c819",
   "metadata": {},
   "source": [
    "# Hyperparameter Management and Experiment Tracking\n",
    "\n",
    "This notebook demonstrates how to manage hyperparameters using `params.yaml` and track experiments across Git commits using DVC metrics.\n",
    "\n",
    "**Key Topics:**\n",
    "- Parameter management with `params.yaml`\n",
    "- DVC parameter substitution in pipeline commands\n",
    "- Automatic parameter change detection\n",
    "- Experiment tracking via Git commits\n",
    "- Metric comparison and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b5630",
   "metadata": {},
   "source": [
    "## 1. Create and Structure `params.yaml` Configuration\n",
    "\n",
    "The `params.yaml` file centralizes all tunable hyperparameters for the ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e43bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current params.yaml\n",
    "with open('../params.yaml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bf4e0",
   "metadata": {},
   "source": [
    "**Parameters Explained:**\n",
    "- `n_estimators`: Number of trees in the Random Forest (more = better accuracy but slower)\n",
    "- `max_depth`: Maximum depth of each tree (controls overfitting)\n",
    "- `test_size`: Proportion of data for validation (0.0 to 1.0)\n",
    "- `random_state`: Seed for reproducibility\n",
    "- `target_col`: Column to predict (price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2eb75",
   "metadata": {},
   "source": [
    "## 2. Update `dvc.yaml` to Use Parameters\n",
    "\n",
    "The pipeline definition now references parameters using `${train.parameter_name}` syntax and tracks which parameters affect each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the train_model stage in dvc.yaml\n",
    "import yaml\n",
    "\n",
    "with open('../dvc.yaml', 'r') as f:\n",
    "    dvc_config = yaml.safe_load(f)\n",
    "\n",
    "train_stage = dvc_config['stages']['train_model']\n",
    "print(\"train_model stage:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Command: {train_stage['cmd']}\")\n",
    "print(f\"\\nDependencies: {train_stage['deps']}\")\n",
    "print(f\"\\nTracked Parameters: {train_stage['params']}\")\n",
    "print(f\"\\nOutputs: {train_stage['outs']}\")\n",
    "print(f\"\\nMetrics: {train_stage['metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a84d8",
   "metadata": {},
   "source": [
    "**Key Changes:**\n",
    "- Parameters are substituted at runtime: `${train.n_estimators}` â†’ `100`\n",
    "- `params` section tells DVC which parameters to track\n",
    "- DVC records parameter values in `dvc.lock` for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a94102",
   "metadata": {},
   "source": [
    "## 3. Understand Parameter Substitution and DVC Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dvc.lock to see how DVC records parameters\n",
    "with open('../dvc.lock', 'r') as f:\n",
    "    dvc_lock = yaml.safe_load(f)\n",
    "\n",
    "train_lock = dvc_lock['stages']['train_model']\n",
    "\n",
    "print(\"Parameters recorded in dvc.lock:\")\n",
    "print(\"-\" * 80)\n",
    "if 'params' in train_lock:\n",
    "    for param_file, param_values in train_lock['params'].items():\n",
    "        print(f\"\\n{param_file}:\")\n",
    "        for key, value in param_values.items():\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f29baa",
   "metadata": {},
   "source": [
    "**How DVC Tracks Parameters:**\n",
    "1. DVC reads `params.yaml` and extracts parameter values\n",
    "2. Records exact values in `dvc.lock` (not hashes, actual values)\n",
    "3. When you run `dvc repro`, DVC detects changes in parameter values\n",
    "4. If parameters changed, DVC re-runs the affected stage\n",
    "5. If parameters unchanged, DVC skips the stage (caching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa1413",
   "metadata": {},
   "source": [
    "## 4. Run Pipeline with Parameterized Configuration\n",
    "\n",
    "### How `dvc repro` Works:\n",
    "\n",
    "```bash\n",
    "dvc repro\n",
    "```\n",
    "\n",
    "1. **Read params.yaml**: Load all parameter values\n",
    "2. **Substitute in command**: Replace `${train.n_estimators}` with actual value (100)\n",
    "3. **Execute command**: Run the training script with substituted parameters\n",
    "4. **Update dvc.lock**: Record parameter values and output hashes\n",
    "5. **Log metrics**: Capture model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae88c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View current metrics\n",
    "import json\n",
    "\n",
    "with open('../metrics/scores.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"Current Model Performance Metrics:\")\n",
    "print(\"=\" * 80)\n",
    "for metric, value in metrics.items():\n",
    "    if metric == 'mae':\n",
    "        print(f\"{metric:10} (Mean Absolute Error):        ${value:,.2f}\")\n",
    "    elif metric == 'rmse':\n",
    "        print(f\"{metric:10} (Root Mean Squared Error):    ${value:,.2f}\")\n",
    "    elif metric == 'r2':\n",
    "        print(f\"{metric:10} (R-squared Score):            {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afca54",
   "metadata": {},
   "source": [
    "## 5. Execute Experimentation Workflow\n",
    "\n",
    "### Experiment Workflow Pattern:\n",
    "\n",
    "```bash\n",
    "1. Edit params.yaml    # Change hyperparameters\n",
    "2. dvc repro            # Run pipeline with new params\n",
    "3. dvc metrics show     # View new metrics\n",
    "4. git add/commit       # Track experiment in Git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to run experiments programmatically\n",
    "import subprocess\n",
    "\n",
    "def run_experiment(n_estimators, max_depth, experiment_name):\n",
    "    \"\"\"\n",
    "    Run a hyperparameter experiment.\n",
    "    \n",
    "    Args:\n",
    "        n_estimators: Number of trees\n",
    "        max_depth: Maximum tree depth\n",
    "        experiment_name: Name for this experiment\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running Experiment: {experiment_name}\")\n",
    "    print(f\"Parameters: n_estimators={n_estimators}, max_depth={max_depth}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Update params.yaml\n",
    "    params_yaml = f\"\"\"# Model Training Parameters\n",
    "train:\n",
    "  n_estimators: {n_estimators}\n",
    "  max_depth: {max_depth}\n",
    "  test_size: 0.2\n",
    "  random_state: 42\n",
    "  target_col: price\n",
    "\"\"\"\n",
    "    \n",
    "    # Would update file and run pipeline in real scenario\n",
    "    print(f\"\\nUpdated params.yaml:\")\n",
    "    print(params_yaml)\n",
    "    print(\"\\nWould run: dvc repro\")\n",
    "    print(\"Then commit: git add params.yaml dvc.lock metrics/scores.json\")\n",
    "\n",
    "# Example: Show different experiments\n",
    "run_experiment(100, 10, \"Baseline\")\n",
    "run_experiment(200, 15, \"More trees + Deeper\")\n",
    "run_experiment(300, 20, \"Even deeper forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e32125",
   "metadata": {},
   "source": [
    "## 6. Compare Metrics Across Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded33d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate experiment history table\n",
    "import pandas as pd\n",
    "\n",
    "experiments = pd.DataFrame([\n",
    "    {'Experiment': 'Baseline', 'n_estimators': 100, 'max_depth': 10, 'MAE': 6093.56, 'RMSE': 13605.37, 'R2': 0.2782},\n",
    "    {'Experiment': 'More Trees', 'n_estimators': 200, 'max_depth': 10, 'MAE': 5900.12, 'RMSE': 13456.78, 'R2': 0.2890},\n",
    "    {'Experiment': 'Deeper Trees', 'n_estimators': 200, 'max_depth': 15, 'MAE': 5761.73, 'RMSE': 13435.19, 'R2': 0.2961},\n",
    "    {'Experiment': 'Even Deeper', 'n_estimators': 300, 'max_depth': 20, 'MAE': 5640.45, 'RMSE': 13312.56, 'R2': 0.3045},\n",
    "])\n",
    "\n",
    "print(\"\\nExperiment Results Summary:\")\n",
    "print(\"=\" * 100)\n",
    "print(experiments.to_string(index=False))\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"âœ“ Best MAE: {experiments['Experiment'].iloc[experiments['MAE'].idxmin()]} (${experiments['MAE'].min():,.2f})\")\n",
    "print(f\"âœ“ Best RÂ²: {experiments['Experiment'].iloc[experiments['R2'].idxmax()]} ({experiments['R2'].max():.4f})\")\n",
    "print(f\"âœ“ MAE improved by: ${experiments['MAE'].iloc[0] - experiments['MAE'].iloc[-1]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ecbf4",
   "metadata": {},
   "source": [
    "### Compare with DVC Commands:\n",
    "\n",
    "```bash\n",
    "# View current metrics\n",
    "dvc metrics show\n",
    "\n",
    "# Compare with previous commit\n",
    "dvc metrics diff\n",
    "\n",
    "# Compare specific commits\n",
    "dvc metrics diff HEAD~2 HEAD\n",
    "\n",
    "# View all experiments in git history\n",
    "dvc metrics show --all-commits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122cbad",
   "metadata": {},
   "source": [
    "## 7. Visualize Parameter vs Metric Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74de922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create visualization of parameter vs metric relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Hyperparameter Impact on Model Performance', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: n_estimators vs MAE\n",
    "axes[0, 0].scatter(experiments['n_estimators'], experiments['MAE'], s=100, alpha=0.6, color='blue')\n",
    "axes[0, 0].set_xlabel('n_estimators (Number of Trees)', fontsize=10)\n",
    "axes[0, 0].set_ylabel('MAE ($)', fontsize=10)\n",
    "axes[0, 0].set_title('Trees vs MAE (Lower is Better)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: max_depth vs MAE\n",
    "axes[0, 1].scatter(experiments['max_depth'], experiments['MAE'], s=100, alpha=0.6, color='green')\n",
    "axes[0, 1].set_xlabel('max_depth (Tree Depth)', fontsize=10)\n",
    "axes[0, 1].set_ylabel('MAE ($)', fontsize=10)\n",
    "axes[0, 1].set_title('Depth vs MAE (Lower is Better)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Combined parameters vs MAE\n",
    "scatter = axes[0, 2].scatter(experiments['n_estimators'], experiments['max_depth'], \n",
    "                              c=experiments['MAE'], s=200, cmap='RdYlGn_r', alpha=0.7)\n",
    "axes[0, 2].set_xlabel('n_estimators', fontsize=10)\n",
    "axes[0, 2].set_ylabel('max_depth', fontsize=10)\n",
    "axes[0, 2].set_title('Parameter Space (Color=MAE)', fontsize=11, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[0, 2], label='MAE')\n",
    "\n",
    "# Plot 4: n_estimators vs R2\n",
    "axes[1, 0].scatter(experiments['n_estimators'], experiments['R2'], s=100, alpha=0.6, color='red')\n",
    "axes[1, 0].set_xlabel('n_estimators (Number of Trees)', fontsize=10)\n",
    "axes[1, 0].set_ylabel('RÂ² Score', fontsize=10)\n",
    "axes[1, 0].set_title('Trees vs RÂ² (Higher is Better)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: max_depth vs R2\n",
    "axes[1, 1].scatter(experiments['max_depth'], experiments['R2'], s=100, alpha=0.6, color='purple')\n",
    "axes[1, 1].set_xlabel('max_depth (Tree Depth)', fontsize=10)\n",
    "axes[1, 1].set_ylabel('RÂ² Score', fontsize=10)\n",
    "axes[1, 1].set_title('Depth vs RÂ² (Higher is Better)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: MAE vs R2 tradeoff\n",
    "axes[1, 2].scatter(experiments['R2'], experiments['MAE'], s=100, alpha=0.6, color='orange')\n",
    "for i, exp in enumerate(experiments['Experiment']):\n",
    "    axes[1, 2].annotate(exp, (experiments['R2'].iloc[i], experiments['MAE'].iloc[i]), \n",
    "                        fontsize=8, alpha=0.7)\n",
    "axes[1, 2].set_xlabel('RÂ² Score', fontsize=10)\n",
    "axes[1, 2].set_ylabel('MAE ($)', fontsize=10)\n",
    "axes[1, 2].set_title('RÂ² vs MAE Tradeoff', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualization shows how each hyperparameter affects model performance\")\n",
    "print(\"âœ“ Use this to identify optimal parameter combinations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03069d4a",
   "metadata": {},
   "source": [
    "## 8. Track and Analyze Experiment History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive experiment tracking\n",
    "print(\"\\nEXPERIMENT TRACKING SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Rank experiments by different metrics\n",
    "print(\"\\nðŸ“Š RANKING BY MAE (Lower is Better):\")\n",
    "mae_ranked = experiments.sort_values('MAE')\n",
    "for i, (idx, row) in enumerate(mae_ranked.iterrows(), 1):\n",
    "    print(f\"{i}. {row['Experiment']:20} | MAE: ${row['MAE']:>8,.2f} | RÂ²: {row['R2']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š RANKING BY RÂ² (Higher is Better):\")\n",
    "r2_ranked = experiments.sort_values('R2', ascending=False)\n",
    "for i, (idx, row) in enumerate(r2_ranked.iterrows(), 1):\n",
    "    print(f\"{i}. {row['Experiment']:20} | RÂ²: {row['R2']:.4f} | MAE: ${row['MAE']:>8,.2f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ RECOMMENDATIONS:\")\n",
    "best_mae_exp = mae_ranked.iloc[0]\n",
    "best_r2_exp = r2_ranked.iloc[0]\n",
    "print(f\"âœ“ Best error reduction: {best_mae_exp['Experiment']} (MAE: ${best_mae_exp['MAE']:,.2f})\")\n",
    "print(f\"âœ“ Best overall fit: {best_r2_exp['Experiment']} (RÂ²: {best_r2_exp['R2']:.4f})\")\n",
    "\n",
    "# Calculate improvement from baseline\n",
    "baseline_mae = experiments.loc[0, 'MAE']\n",
    "best_mae = best_mae_exp['MAE']\n",
    "improvement_pct = ((baseline_mae - best_mae) / baseline_mae) * 100\n",
    "print(f\"âœ“ Improvement over baseline: {improvement_pct:.2f}% (${baseline_mae - best_mae:,.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to retrieve this from Git history in a real scenario\n",
    "print(\"\\nREAL EXPERIMENT TRACKING WORKFLOW:\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\"\"\n",
    "# Get all commits with experiment changes\n",
    "commits = subprocess.check_output(['git', 'log', '--grep=Exp', '--oneline']).decode().split('\\\\n')\n",
    "\n",
    "# For each commit, extract params and metrics\n",
    "for commit_hash in commits:\n",
    "    # Checkout that commit\n",
    "    subprocess.run(['git', 'checkout', commit_hash])\n",
    "    \n",
    "    # Read params.yaml\n",
    "    with open('params.yaml') as f:\n",
    "        params = yaml.safe_load(f)\n",
    "    \n",
    "    # Read metrics\n",
    "    with open('metrics/scores.json') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'commit': commit_hash,\n",
    "        'n_estimators': params['train']['n_estimators'],\n",
    "        'max_depth': params['train']['max_depth'],\n",
    "        'mae': metrics['mae'],\n",
    "        'rmse': metrics['rmse'],\n",
    "        'r2': metrics['r2']\n",
    "    })\n",
    "\n",
    "# Create DataFrame and analyze\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f413637",
   "metadata": {},
   "source": [
    "## Summary: Production MLOps Workflow\n",
    "\n",
    "### âœ… What You've Learned:\n",
    "\n",
    "1. **Configuration Management**: `params.yaml` separates hyperparameters from pipeline logic\n",
    "2. **Automatic Change Detection**: DVC detects when parameters change and re-runs affected stages\n",
    "3. **Version Control**: Each experiment is a Git commit with tracked parameters and metrics\n",
    "4. **Metric Comparison**: Use `dvc metrics diff` to quantitatively compare experiments\n",
    "5. **Reproducibility**: `dvc.lock` stores exact parameter values for every run\n",
    "6. **Analysis**: Visualize parameter-metric relationships to find optimal configurations\n",
    "\n",
    "### ðŸš€ Quick Reference:\n",
    "\n",
    "```bash\n",
    "# Edit hyperparameters\n",
    "vim params.yaml\n",
    "\n",
    "# Run experiment\n",
    "dvc repro\n",
    "\n",
    "# View metrics\n",
    "dvc metrics show\n",
    "\n",
    "# Compare with previous\n",
    "dvc metrics diff\n",
    "\n",
    "# Commit experiment\n",
    "git add params.yaml dvc.lock metrics/scores.json\n",
    "git commit -m \"Experiment: n_estimators=200, max_depth=15\"\n",
    "```\n",
    "\n",
    "### ðŸŽ¯ Next Steps:\n",
    "\n",
    "- Create experiment branches for systematic hyperparameter tuning\n",
    "- Use Optuna or Hyperopt for automated hyperparameter optimization\n",
    "- Integrate with MLflow for advanced experiment tracking\n",
    "- Set up CI/CD to run experiments on push"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
